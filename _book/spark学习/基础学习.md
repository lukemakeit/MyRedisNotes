### Spark 和 Hadoop

Haddoop: 主要适用于 一次性数据计算。框架在处理数据时，会从存储设备中读取数据，进行逻辑操作，然后将处理结果重新存储到介质中；

![image-20220305212434328](https://my-typora-pictures-1252258460.cos.ap-guangzhou.myqcloud.com/img/image-20220305212434328.png)

缺点:

- 计算引擎简单，只有mapper和reducer;
- 作业之间的交互 靠磁盘IO，上一个的结果保存到磁盘中，作为下一个任务的输入;

Spark和Hadoop的根本差异是多个作业之间的数据通信问题: Spark多个作业之间的数据通信基于内存，而Hadoop基于磁盘。

SparkTask的启动时间快。Spark采用fork线程的方式，而Hadoop采用创建新进程的方式。

Spark只有在shuffle的时候将数据写入磁盘，而Hadoop中多个MR作业之间的数据交互都依赖磁盘交互。

Spark可以快速在内存中对数据集进行多次迭代，来支持复杂的数据挖掘算法和图形计算算法。

Spark并通过并行计算DAG图的优化，减少了不同任务之间的依赖，降低了延迟等待时间;

缺点：

- 内存占用资源更大。如果因为内存不足导致作业失败，此时可以使用MapReduce。

### Spark 核心模块

![image-20220305213349790](https://my-typora-pictures-1252258460.cos.ap-guangzhou.myqcloud.com/img/image-20220305213349790.png)

### 基础知识

RDD的两种操作是:

- **转化操作**：返回一个新的RDD的操作;

- **行动操作**:向程序返回结果或把结果写入外部系统的操作，会触发实际的计算;

**转化操作和行动操作的示例**：使用filter函数过滤“python”、“java”相关的行，使用union函数合并这些行，使用take函数触发计算。

```python
>>> lines = sc.textFile("file:///home/grid/spark3/README.md")
>>> java_lines = lines.filter(lambda x: 'Java' in x)
>>> java_lines.first()
u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'
>>> union_lines = python_lines.union(java_lines) 
>>> union_lines.take(10)
[u'high-level APIs ', u'## Interactive Python Shell',...]
```

**常见的RDD转化操作:**

- map()：将函数应用于 RDD 中的每个元素，将返回值构成新的 RDD;

- flatMap()：对RDD中的item执行同一个操作以后得到一个list，然后以平铺的方式把这些list里所有的结果组成新的list;

- filter()：返回一个由通过传给 filter()的函数的元素组成的 RDD;

  ```python
  numbersRDD = sc.parallelize(range(1,10+1))
  print(numbersRDD.collect())
  #map()对RDD的每一个item都执行同一个操作
  squaresRDD = numbersRDD.map(lambda x: x**2)  # Square every number
  print(squaresRDD.collect())
  #filter()筛选出来满足条件的item
  filteredRDD = numbersRDD.filter(lambda x: x % 2 == 0)  # Only the evens
  print(filteredRDD.collect())
  
  #Output:
  #[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
  #[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]
  #[2, 4, 6, 8, 10]
  
  sentencesRDD=sc.parallelize(['Hello world','My name is Patrick'])
  wordsRDD=sentencesRDD.flatMap(lambda sentence: sentence.split(" "))
  print(wordsRDD.collect())
  print(wordsRDD.count())
  
  #Output:
  #['Hello', 'world', 'My', 'name', 'is', 'Patrick']
  #6
  #这里如果使用map的结果是[[‘Hello’, ‘world’], [‘My’, ‘name’, ‘is’, ‘Patrick’]]
  ```

  ```python
  #flatmap即对应Python里的如下操作
  l = ['Hello world', 'My name is Patrick']
  ll = []
  for sentence in l:
      ll = ll + sentence.split(" ")  #+号作用,two list拼接
  ll
  ```

- distinct()：去重

- sample(withReplacement, fraction, [seed])：对 RDD 采样，以及是否替换

- union()：生成一个包含两个 RDD 中所有元素的 RDD

- intersection()：求两个 RDD 共同的元素的 RDD

- subtract()：移除一个 RDD 中的内容（例如移除训练数据）

- cartesian()：与另一个 RDD 的笛卡儿积;

- sortBy(): 对RDD中的item进行排序;

更复杂的结构和运算:

- reduceByKey(): 对所有有着相同key的items执行reduce操作
- groupByKey(): 返回类似(key, listOfValues)元组的RDD，后面的value List 是同一个key下面的
- sortByKey(): 按照key排序
- countByKey(): 按照key去对item个数进行统计
- collectAsMap(): 和collect有些类似，但是返回的是k-v的字典

**常见的RDD行动操作:**

- collect() 返回 RDD 中的所有元素
- count() RDD 中的元素个数
- countByValue() 各元素在 RDD 中出现的次数
- take(num) 从 RDD 中返回 num 个元素
- top(num) 从 RDD 中返回最前面的 num个元素
- takeOrdered(num)(ordering)从 RDD 中按照提供的顺序返回最前面的 num 个元素
- takeSample(withReplacement, num, [seed])从 RDD 中返回任意一些元素
- reduce(func) 并 行 整 合 RDD 中 所 有 数 据（例如 sum）
- fold(zero)(func) 和 reduce() 一 样， 但 是 需 要提供初始值
- aggregate(zeroValue)(seqOp, combOp)和 reduce() 相 似， 但 是 通 常返回不同类型的函数
- foreach(func) 对 RDD 中的每个元素使用给定的函数;

```python
//获取全部的结果
lines.collect()

//获取第一行
lines.first()

//获取前5行
lines.take(5)

//过滤只有Java的行
java_lines = lines.filter(lambda x: 'Java' in x)

def filter_Java_lines(s):
  return 'Java' in s
java_lines = lines.filter(filter_java_lines)
java_lines.collect()
```

